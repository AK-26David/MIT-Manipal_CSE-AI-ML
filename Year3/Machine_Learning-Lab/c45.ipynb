{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decision Tree:\n",
      "{'Outlook': {'Sunny': {'Humidity': {'High': 'No', 'Low': 'Yes'}}, 'Overcast': 'Yes', 'Rainy': {'Humidity': {'High': {'Wind': {'Weak': 'Yes', 'Strong': 'No'}}, 'Low': 'No'}}}}\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Calculate the Entropy of a dataset\n",
    "def entropy(data):\n",
    "    class_counts = data.iloc[:, -1].value_counts()\n",
    "    probabilities = class_counts / len(data)\n",
    "    return -np.sum(probabilities * np.log2(probabilities))\n",
    "\n",
    "# Calculate the Intrinsic Information of a feature\n",
    "def intrinsic_info(data, feature):\n",
    "    feature_values = data[feature].value_counts() / len(data)\n",
    "    return -np.sum(feature_values * np.log2(feature_values))\n",
    "\n",
    "# Calculate the Gain Ratio of a feature\n",
    "def gain_ratio(data, feature):\n",
    "    # Calculate Information Gain\n",
    "    gain = entropy(data)\n",
    "    feature_values = data[feature].value_counts()\n",
    "    weighted_entropy = 0\n",
    "    for value, count in feature_values.items():\n",
    "        subset = data[data[feature] == value]\n",
    "        weighted_entropy += (count / len(data)) * entropy(subset)\n",
    "    \n",
    "    information_gain = gain - weighted_entropy\n",
    "    \n",
    "    # Calculate Intrinsic Information\n",
    "    intrinsic = intrinsic_info(data, feature)\n",
    "    \n",
    "    # Gain Ratio\n",
    "    return information_gain / intrinsic if intrinsic != 0 else 0\n",
    "\n",
    "# Handle continuous features by choosing the best threshold to split\n",
    "def best_split_continuous(data, feature):\n",
    "    sorted_data = data.sort_values(by=feature)\n",
    "    max_gain = 0\n",
    "    best_threshold = None\n",
    "    for i in range(1, len(sorted_data)):\n",
    "        threshold = (sorted_data.iloc[i - 1][feature] + sorted_data.iloc[i][feature]) / 2\n",
    "        left_subset = data[data[feature] <= threshold]\n",
    "        right_subset = data[data[feature] > threshold]\n",
    "        \n",
    "        gain_left = entropy(left_subset)\n",
    "        gain_right = entropy(right_subset)\n",
    "        \n",
    "        total_entropy = (len(left_subset) / len(data)) * gain_left + (len(right_subset) / len(data)) * gain_right\n",
    "        gain = entropy(data) - total_entropy\n",
    "        if gain > max_gain:\n",
    "            max_gain = gain\n",
    "            best_threshold = threshold\n",
    "    \n",
    "    return best_threshold, max_gain\n",
    "\n",
    "# C4.5 Algorithm for building the decision tree\n",
    "def c45(data, features):\n",
    "    # If all rows have the same class, return the class\n",
    "    if len(data.iloc[:, -1].unique()) == 1:\n",
    "        return data.iloc[0, -1]\n",
    "    \n",
    "    # If no features left to split on, return the majority class\n",
    "    if len(features) == 0:\n",
    "        return data.iloc[:, -1].mode()[0]\n",
    "    \n",
    "    # Find the feature with the highest Gain Ratio\n",
    "    gains = {feature: gain_ratio(data, feature) for feature in features}\n",
    "    best_feature = max(gains, key=gains.get)\n",
    "    \n",
    "    # Create the tree\n",
    "    tree = {best_feature: {}}\n",
    "    \n",
    "    # Handle continuous features\n",
    "    if isinstance(data[best_feature].iloc[0], (int, float)):\n",
    "        best_threshold, max_gain = best_split_continuous(data, best_feature)\n",
    "        tree[best_feature]['<= ' + str(best_threshold)] = c45(data[data[best_feature] <= best_threshold], features)\n",
    "        tree[best_feature]['> ' + str(best_threshold)] = c45(data[data[best_feature] > best_threshold], features)\n",
    "    else:\n",
    "        # Split the data based on the best feature\n",
    "        feature_values = data[best_feature].unique()\n",
    "        for value in feature_values:\n",
    "            subset = data[data[best_feature] == value]\n",
    "            tree[best_feature][value] = c45(subset, [f for f in features if f != best_feature])\n",
    "    \n",
    "    return tree\n",
    "\n",
    "# Example dataset (replace with your own data)\n",
    "data = pd.DataFrame({\n",
    "    'Outlook': ['Sunny', 'Sunny', 'Overcast', 'Rainy', 'Rainy', 'Rainy', 'Overcast', 'Sunny', 'Sunny', 'Rainy'],\n",
    "    'Temperature': ['Hot', 'Hot', 'Hot', 'Mild', 'Cool', 'Cool', 'Mild', 'Mild', 'Cool', 'Mild'],\n",
    "    'Humidity': ['High', 'High', 'High', 'High', 'High', 'Low', 'Low', 'Low', 'Low', 'High'],\n",
    "    'Wind': ['Weak', 'Strong', 'Weak', 'Weak', 'Weak', 'Weak', 'Strong', 'Strong', 'Weak', 'Strong'],\n",
    "    'PlayTennis': ['No', 'No', 'Yes', 'Yes', 'Yes', 'No', 'Yes', 'Yes', 'Yes', 'No']\n",
    "})\n",
    "\n",
    "# List of features (excluding the target column)\n",
    "features = data.columns[:-1].tolist()\n",
    "\n",
    "# Build the Decision Tree using C4.5\n",
    "tree = c45(data, features)\n",
    "print(\"Decision Tree:\")\n",
    "print(tree)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
