{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Entity Resolution #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/11/05 18:48:09 WARN Utils: Your hostname, Arnavs-MacBook-Air.local resolves to a loopback address: 127.0.0.1; using 172.20.10.2 instead (on interface en0)\n",
      "24/11/05 18:48:09 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "24/11/05 18:48:09 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "[Stage 0:>                                                          (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----------+--------------------------------------------------------+\n",
      "|id |name       |name_norm                                               |\n",
      "+---+-----------+--------------------------------------------------------+\n",
      "|1  |John Doe   |(1000,[701,818],[0.7071067811865475,0.7071067811865475])|\n",
      "|2  |Jon Doe    |(1000,[818,828],[0.7071067811865475,0.7071067811865475])|\n",
      "|3  |Johnny Doe |(1000,[818,998],[0.7071067811865475,0.7071067811865475])|\n",
      "|4  |Jane Smith |(1000,[685,901],[0.7071067811865475,0.7071067811865475])|\n",
      "|5  |J Smith    |(1000,[660,685],[0.7071067811865475,0.7071067811865475])|\n",
      "|6  |Janet Smith|(1000,[204,685],[0.7071067811865475,0.7071067811865475])|\n",
      "|7  |Doe John   |(1000,[701,818],[0.7071067811865475,0.7071067811865475])|\n",
      "|8  |Smith Jane |(1000,[685,901],[0.7071067811865475,0.7071067811865475])|\n",
      "+---+-----------+--------------------------------------------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, lower, trim, regexp_replace\n",
    "from pyspark.ml.feature import Tokenizer, StopWordsRemover, HashingTF, Normalizer\n",
    "\n",
    "# Initialize Spark Session\n",
    "spark = SparkSession.builder.appName(\"EntityResolutionHardcoded\").getOrCreate()\n",
    "\n",
    "# Hardcoded data simulating slight variations in names\n",
    "data = [\n",
    "    (1, \"John Doe\"),\n",
    "    (2, \"Jon Doe\"),\n",
    "    (3, \"Johnny Doe\"),\n",
    "    (4, \"Jane Smith\"),\n",
    "    (5, \"J Smith\"),\n",
    "    (6, \"Janet Smith\"),\n",
    "    (7, \"Doe John\"),\n",
    "    (8, \"Smith Jane\")\n",
    "]\n",
    "\n",
    "# Create DataFrame from hardcoded data\n",
    "columns = [\"id\", \"name\"]\n",
    "df = spark.createDataFrame(data, columns)\n",
    "\n",
    "# Data Cleaning and Preprocessing\n",
    "# Step 1: Normalize text (convert to lowercase, remove punctuation and whitespace)\n",
    "df_cleaned = df.withColumn(\"name_normalized\", lower(trim(col(\"name\"))))\n",
    "df_cleaned = df_cleaned.withColumn(\"name_normalized\", regexp_replace(col(\"name_normalized\"), \"[^a-zA-Z0-9\\\\s]\", \"\"))\n",
    "\n",
    "# Step 2: Tokenization\n",
    "tokenizer = Tokenizer(inputCol=\"name_normalized\", outputCol=\"name_tokens\")\n",
    "df_tokenized = tokenizer.transform(df_cleaned)\n",
    "\n",
    "# Step 3: Remove stop words\n",
    "stop_words_remover = StopWordsRemover(inputCol=\"name_tokens\", outputCol=\"name_filtered_tokens\")\n",
    "df_filtered = stop_words_remover.transform(df_tokenized)\n",
    "\n",
    "# Step 4: Vectorize using HashingTF\n",
    "hashing_tf = HashingTF(inputCol=\"name_filtered_tokens\", outputCol=\"name_tf\", numFeatures=1000)\n",
    "df_vectorized = hashing_tf.transform(df_filtered)\n",
    "\n",
    "# Step 5: Normalize the vectors\n",
    "normalizer = Normalizer(inputCol=\"name_tf\", outputCol=\"name_norm\")\n",
    "df_normalized = normalizer.transform(df_vectorized)\n",
    "\n",
    "# Select only relevant columns for entity resolution\n",
    "df_final = df_normalized.select(\"id\", \"name\", \"name_norm\")\n",
    "df_final.show(truncate=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----------+---+----------+----------------+\n",
      "|id |name      |id |name      |similarity_score|\n",
      "+---+----------+---+----------+----------------+\n",
      "|1  |John Doe  |7  |Doe John  |1.0             |\n",
      "|4  |Jane Smith|8  |Smith Jane|1.0             |\n",
      "+---+----------+---+----------+----------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.types import FloatType\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "# Define a UDF to compute cosine similarity between two vectors\n",
    "def cosine_similarity(v1, v2):\n",
    "    return float(v1.dot(v2) / (v1.norm(2) * v2.norm(2)))\n",
    "\n",
    "cosine_similarity_udf = F.udf(cosine_similarity, FloatType())\n",
    "\n",
    "# Self-join the DataFrame to get record pairs\n",
    "df_pairs = df_final.alias(\"a\").join(df_final.alias(\"b\"), F.col(\"a.id\") < F.col(\"b.id\"))\n",
    "\n",
    "# Calculate similarity score\n",
    "df_similarity = df_pairs.withColumn(\"similarity_score\", cosine_similarity_udf(col(\"a.name_norm\"), col(\"b.name_norm\")))\n",
    "\n",
    "# Filter pairs with a high similarity score\n",
    "similarity_threshold = 0.8\n",
    "df_matches = df_similarity.filter(col(\"similarity_score\") >= similarity_threshold)\n",
    "\n",
    "df_matches.select(\"a.id\", \"a.name\", \"b.id\", \"b.name\", \"similarity_score\").show(truncate=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 1.00\n",
      "Recall: 0.50\n",
      "F1 Score: 0.67\n"
     ]
    }
   ],
   "source": [
    "# Hardcoded ground truth\n",
    "ground_truth_data = [\n",
    "    (1, 2),\n",
    "    (4, 5),\n",
    "    (1, 7),\n",
    "    (4, 8)\n",
    "]\n",
    "ground_truth_columns = [\"id1\", \"id2\"]\n",
    "ground_truth = spark.createDataFrame(ground_truth_data, ground_truth_columns)\n",
    "\n",
    "# Join matched pairs with ground truth to get true positives\n",
    "true_positives = df_matches.join(ground_truth, (df_matches[\"a.id\"] == ground_truth[\"id1\"]) & (df_matches[\"b.id\"] == ground_truth[\"id2\"]), \"inner\")\n",
    "\n",
    "# Calculate counts\n",
    "tp_count = true_positives.count()\n",
    "predicted_match_count = df_matches.count()\n",
    "actual_match_count = ground_truth.count()\n",
    "\n",
    "# Calculate Precision, Recall, and F1-Score\n",
    "precision = tp_count / predicted_match_count if predicted_match_count > 0 else 0\n",
    "recall = tp_count / actual_match_count if actual_match_count > 0 else 0\n",
    "f1_score = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0\n",
    "\n",
    "print(f\"Precision: {precision:.2f}\")\n",
    "print(f\"Recall: {recall:.2f}\")\n",
    "print(f\"F1 Score: {f1_score:.2f}\")\n",
    "\n",
    "# Stop the Spark session\n",
    "spark.stop()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
