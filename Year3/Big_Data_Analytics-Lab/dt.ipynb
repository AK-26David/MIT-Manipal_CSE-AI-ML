{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decision Trees #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col\n",
    "from pyspark.sql.functions import mean\n",
    "from pyspark.ml.feature import StringIndexer , OneHotEncoder, VectorAssembler\n",
    "from pyspark.ml.classification import DecisionTreeClassifier\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+------+-------+-------------+----+\n",
      "|   age|income|student|credit_rating|buys|\n",
      "+------+------+-------+-------------+----+\n",
      "| young|  high|     no|         fair|  no|\n",
      "| young|  high|     no|    excellent|  no|\n",
      "|middle|  high|     no|         fair| yes|\n",
      "|senior|medium|     no|         fair| yes|\n",
      "|senior|   low|    yes|         fair| yes|\n",
      "|senior|   low|    yes|    excellent|  no|\n",
      "|middle|   low|    yes|    excellent| yes|\n",
      "| young|medium|     no|         fair|  no|\n",
      "| young|   low|    yes|         fair| yes|\n",
      "|senior|medium|    yes|         fair| yes|\n",
      "| young|medium|    yes|    excellent| yes|\n",
      "|middle|medium|     no|    excellent| yes|\n",
      "|middle|  high|    yes|         fair| yes|\n",
      "|senior|medium|     no|    excellent|  no|\n",
      "+------+------+-------+-------------+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sc = SparkSession.builder.appName(\"Decision Trees\").getOrCreate()\n",
    "data_path = \"/Users/arnavkarnik/Documents/MIT-Manipal_CSE-AI-ML/Year3/Big_Data_Analytics-Lab/data.csv\"\n",
    "df = sc.read.csv(data_path , header=True, inferSchema= True)\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data after handling missing values:\n",
      "+------+------+-------+-------------+----+\n",
      "|   age|income|student|credit_rating|buys|\n",
      "+------+------+-------+-------------+----+\n",
      "| young|  high|     no|         fair|  no|\n",
      "| young|  high|     no|    excellent|  no|\n",
      "|middle|  high|     no|         fair| yes|\n",
      "|senior|medium|     no|         fair| yes|\n",
      "|senior|   low|    yes|         fair| yes|\n",
      "|senior|   low|    yes|    excellent|  no|\n",
      "|middle|   low|    yes|    excellent| yes|\n",
      "| young|medium|     no|         fair|  no|\n",
      "| young|   low|    yes|         fair| yes|\n",
      "|senior|medium|    yes|         fair| yes|\n",
      "| young|medium|    yes|    excellent| yes|\n",
      "|middle|medium|     no|    excellent| yes|\n",
      "|middle|  high|    yes|         fair| yes|\n",
      "|senior|medium|     no|    excellent|  no|\n",
      "+------+------+-------+-------------+----+\n",
      "\n",
      "Data after encoding categorical features:\n",
      "+------+------+-------+-------------+----+---------+-------------+------------+--------------+-------------+---------------+-------------------+---------------------+----------+\n",
      "|   age|income|student|credit_rating|buys|age_index|  age_encoded|income_index|income_encoded|student_index|student_encoded|credit_rating_index|credit_rating_encoded|buys_index|\n",
      "+------+------+-------+-------------+----+---------+-------------+------------+--------------+-------------+---------------+-------------------+---------------------+----------+\n",
      "| young|  high|     no|         fair|  no|      1.0|(2,[1],[1.0])|         1.0| (2,[1],[1.0])|          0.0|  (1,[0],[1.0])|                0.0|        (1,[0],[1.0])|       1.0|\n",
      "| young|  high|     no|    excellent|  no|      1.0|(2,[1],[1.0])|         1.0| (2,[1],[1.0])|          0.0|  (1,[0],[1.0])|                1.0|            (1,[],[])|       1.0|\n",
      "|middle|  high|     no|         fair| yes|      2.0|    (2,[],[])|         1.0| (2,[1],[1.0])|          0.0|  (1,[0],[1.0])|                0.0|        (1,[0],[1.0])|       0.0|\n",
      "|senior|medium|     no|         fair| yes|      0.0|(2,[0],[1.0])|         0.0| (2,[0],[1.0])|          0.0|  (1,[0],[1.0])|                0.0|        (1,[0],[1.0])|       0.0|\n",
      "|senior|   low|    yes|         fair| yes|      0.0|(2,[0],[1.0])|         2.0|     (2,[],[])|          1.0|      (1,[],[])|                0.0|        (1,[0],[1.0])|       0.0|\n",
      "|senior|   low|    yes|    excellent|  no|      0.0|(2,[0],[1.0])|         2.0|     (2,[],[])|          1.0|      (1,[],[])|                1.0|            (1,[],[])|       1.0|\n",
      "|middle|   low|    yes|    excellent| yes|      2.0|    (2,[],[])|         2.0|     (2,[],[])|          1.0|      (1,[],[])|                1.0|            (1,[],[])|       0.0|\n",
      "| young|medium|     no|         fair|  no|      1.0|(2,[1],[1.0])|         0.0| (2,[0],[1.0])|          0.0|  (1,[0],[1.0])|                0.0|        (1,[0],[1.0])|       1.0|\n",
      "| young|   low|    yes|         fair| yes|      1.0|(2,[1],[1.0])|         2.0|     (2,[],[])|          1.0|      (1,[],[])|                0.0|        (1,[0],[1.0])|       0.0|\n",
      "|senior|medium|    yes|         fair| yes|      0.0|(2,[0],[1.0])|         0.0| (2,[0],[1.0])|          1.0|      (1,[],[])|                0.0|        (1,[0],[1.0])|       0.0|\n",
      "| young|medium|    yes|    excellent| yes|      1.0|(2,[1],[1.0])|         0.0| (2,[0],[1.0])|          1.0|      (1,[],[])|                1.0|            (1,[],[])|       0.0|\n",
      "|middle|medium|     no|    excellent| yes|      2.0|    (2,[],[])|         0.0| (2,[0],[1.0])|          0.0|  (1,[0],[1.0])|                1.0|            (1,[],[])|       0.0|\n",
      "|middle|  high|    yes|         fair| yes|      2.0|    (2,[],[])|         1.0| (2,[1],[1.0])|          1.0|      (1,[],[])|                0.0|        (1,[0],[1.0])|       0.0|\n",
      "|senior|medium|     no|    excellent|  no|      0.0|(2,[0],[1.0])|         0.0| (2,[0],[1.0])|          0.0|  (1,[0],[1.0])|                1.0|            (1,[],[])|       1.0|\n",
      "+------+------+-------+-------------+----+---------+-------------+------------+--------------+-------------+---------------+-------------------+---------------------+----------+\n",
      "\n",
      "Final Processed Data:\n",
      "+--------------------+----------+\n",
      "|            features|buys_index|\n",
      "+--------------------+----------+\n",
      "|[0.0,1.0,0.0,1.0,...|       1.0|\n",
      "|[0.0,1.0,0.0,1.0,...|       1.0|\n",
      "|[0.0,0.0,0.0,1.0,...|       0.0|\n",
      "|[1.0,0.0,1.0,0.0,...|       0.0|\n",
      "| (6,[0,5],[1.0,1.0])|       0.0|\n",
      "|       (6,[0],[1.0])|       1.0|\n",
      "|           (6,[],[])|       0.0|\n",
      "|[0.0,1.0,1.0,0.0,...|       1.0|\n",
      "| (6,[1,5],[1.0,1.0])|       0.0|\n",
      "|[1.0,0.0,1.0,0.0,...|       0.0|\n",
      "| (6,[1,2],[1.0,1.0])|       0.0|\n",
      "| (6,[2,4],[1.0,1.0])|       0.0|\n",
      "| (6,[3,5],[1.0,1.0])|       0.0|\n",
      "|[1.0,0.0,1.0,0.0,...|       1.0|\n",
      "+--------------------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Step 1: Handle Missing Values\n",
    "\n",
    "# Define categorical and numerical columns\n",
    "categorical_columns = [\"age\", \"income\", \"student\", \"credit_rating\"]\n",
    "target_column = \"buys\"\n",
    "numerical_columns = [col_name for col_name, dtype in df.dtypes if dtype in (\"int\", \"double\")]\n",
    "\n",
    "# Fill missing values for categorical columns with the mode\n",
    "for column in categorical_columns:\n",
    "    mode_value = df.groupBy(column).count().orderBy(\"count\", ascending=False).first()[0]\n",
    "    df = df.fillna({column: mode_value})\n",
    "\n",
    "# Fill missing values for numerical columns with the mean\n",
    "for column in numerical_columns:\n",
    "    mean_value = df.select(mean(col(column))).first()[0]\n",
    "    df = df.fillna({column: mean_value})\n",
    "\n",
    "print(\"Data after handling missing values:\")\n",
    "df.show()\n",
    "\n",
    "# Step 2: Encode Categorical Features\n",
    "\n",
    "# Manually apply StringIndexer and OneHotEncoder transformations for each categorical column\n",
    "indexers = {}\n",
    "encoders = {}\n",
    "for column in categorical_columns:\n",
    "    # Index the column\n",
    "    indexer = StringIndexer(inputCol=column, outputCol=column + \"_index\").fit(df)\n",
    "    df = indexer.transform(df)\n",
    "    indexers[column] = indexer\n",
    "    \n",
    "    # One-hot encode the indexed column\n",
    "    encoder = OneHotEncoder(inputCol=column + \"_index\", outputCol=column + \"_encoded\")\n",
    "    df = encoder.fit(df).transform(df)\n",
    "    encoders[column] = encoder\n",
    "\n",
    "# Index the target column\n",
    "target_indexer = StringIndexer(inputCol=target_column, outputCol=target_column + \"_index\").fit(df)\n",
    "df = target_indexer.transform(df)\n",
    "\n",
    "# Display data after encoding\n",
    "print(\"Data after encoding categorical features:\")\n",
    "df.show()\n",
    "\n",
    "# Step 3: Assemble Features\n",
    "\n",
    "# Assemble all encoded features and numerical columns into a single vector\n",
    "assembler = VectorAssembler(\n",
    "    inputCols=[column + \"_encoded\" for column in categorical_columns] + numerical_columns,\n",
    "    outputCol=\"features\"\n",
    ")\n",
    "df = assembler.transform(df)\n",
    "\n",
    "# Select final columns for modeling\n",
    "final_df = df.select(\"features\", target_column + \"_index\")\n",
    "\n",
    "print(\"Final Processed Data:\")\n",
    "final_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 0.50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/11/05 17:21:37 WARN DecisionTreeMetadata: DecisionTree reducing maxBins from 32 to 10 (= number of training instances)\n"
     ]
    }
   ],
   "source": [
    "# Split the data into training and test sets\n",
    "train_df, test_df = final_df.randomSplit([0.8, 0.2], seed=42)\n",
    "\n",
    "# Step 2: Train a Decision Tree Model\n",
    "\n",
    "# Initialize the Decision Tree Classifier\n",
    "dt = DecisionTreeClassifier(labelCol=target_column + \"_index\", featuresCol=\"features\", maxDepth=5)\n",
    "\n",
    "# Train the model\n",
    "dt_model = dt.fit(train_df)\n",
    "\n",
    "# Step 3: Make Predictions and Evaluate the Model\n",
    "\n",
    "# Make predictions on the test set\n",
    "predictions = dt_model.transform(test_df)\n",
    "\n",
    "# Evaluate the model using accuracy\n",
    "evaluator = MulticlassClassificationEvaluator(\n",
    "    labelCol=target_column + \"_index\", predictionCol=\"prediction\", metricName=\"accuracy\"\n",
    ")\n",
    "accuracy = evaluator.evaluate(predictions)\n",
    "\n",
    "print(f\"Test Accuracy: {accuracy:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.50\n",
      "Precision: 0.83\n",
      "Recall: 0.50\n"
     ]
    }
   ],
   "source": [
    "# Initialize evaluator for different metrics\n",
    "evaluator_accuracy = MulticlassClassificationEvaluator(\n",
    "    labelCol=target_column + \"_index\", predictionCol=\"prediction\", metricName=\"accuracy\"\n",
    ")\n",
    "evaluator_precision = MulticlassClassificationEvaluator(\n",
    "    labelCol=target_column + \"_index\", predictionCol=\"prediction\", metricName=\"weightedPrecision\"\n",
    ")\n",
    "evaluator_recall = MulticlassClassificationEvaluator(\n",
    "    labelCol=target_column + \"_index\", predictionCol=\"prediction\", metricName=\"weightedRecall\"\n",
    ")\n",
    "\n",
    "# Calculate accuracy, precision, and recall\n",
    "accuracy = evaluator_accuracy.evaluate(predictions)\n",
    "precision = evaluator_precision.evaluate(predictions)\n",
    "recall = evaluator_recall.evaluate(predictions)\n",
    "\n",
    "print(f\"Accuracy: {accuracy:.2f}\")\n",
    "print(f\"Precision: {precision:.2f}\")\n",
    "print(f\"Recall: {recall:.2f}\")\n",
    "\n",
    "# Stop the Spark session\n",
    "sc.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/11/05 17:21:40 WARN SparkSession: Using an existing Spark session; only runtime SQL configurations will take effect.\n",
      "24/11/05 17:21:41 WARN DecisionTreeMetadata: DecisionTree reducing maxBins from 32 to 10 (= number of training instances)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.75\n",
      "Precision: 0.88\n",
      "Recall: 0.75\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.ml.classification import RandomForestClassifier\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "from pyspark.ml.feature import StringIndexer, VectorAssembler, OneHotEncoder\n",
    "from pyspark.sql.functions import mean, col\n",
    "\n",
    "# Initialize Spark Session\n",
    "spark = SparkSession.builder.appName(\"RandomForestEvaluation\").getOrCreate()\n",
    "\n",
    "# Load the CSV file into a DataFrame (path/to/your/data.csv should be replaced with your actual file path)\n",
    "data_path = \"/Users/arnavkarnik/Documents/MIT-Manipal_CSE-AI-ML/Year3/Big_Data_Analytics-Lab/data.csv\"\n",
    "df = spark.read.csv(data_path, header=True, inferSchema=True)\n",
    "\n",
    "# Data Preparation (Refer to previous code for full processing steps)\n",
    "\n",
    "# Define categorical and numerical columns\n",
    "categorical_columns = [\"age\", \"income\", \"student\", \"credit_rating\"]\n",
    "target_column = \"buys\"\n",
    "numerical_columns = [col_name for col_name, dtype in df.dtypes if dtype in (\"int\", \"double\")]\n",
    "\n",
    "# Fill missing values (mode for categorical, mean for numerical)\n",
    "for column in categorical_columns:\n",
    "    mode_value = df.groupBy(column).count().orderBy(\"count\", ascending=False).first()[0]\n",
    "    df = df.fillna({column: mode_value})\n",
    "\n",
    "for column in numerical_columns:\n",
    "    mean_value = df.select(mean(col(column))).first()[0]\n",
    "    df = df.fillna({column: mean_value})\n",
    "\n",
    "# Encode Categorical Features\n",
    "for column in categorical_columns:\n",
    "    indexer = StringIndexer(inputCol=column, outputCol=column + \"_index\").fit(df)\n",
    "    df = indexer.transform(df)\n",
    "    encoder = OneHotEncoder(inputCol=column + \"_index\", outputCol=column + \"_encoded\")\n",
    "    df = encoder.fit(df).transform(df)\n",
    "\n",
    "target_indexer = StringIndexer(inputCol=target_column, outputCol=target_column + \"_index\").fit(df)\n",
    "df = target_indexer.transform(df)\n",
    "\n",
    "# Assemble Features\n",
    "assembler = VectorAssembler(\n",
    "    inputCols=[column + \"_encoded\" for column in categorical_columns] + numerical_columns,\n",
    "    outputCol=\"features\"\n",
    ")\n",
    "df = assembler.transform(df)\n",
    "\n",
    "# Select final DataFrame\n",
    "final_df = df.select(\"features\", target_column + \"_index\")\n",
    "\n",
    "# Split data into training and test sets\n",
    "train_df, test_df = final_df.randomSplit([0.8, 0.2], seed=42)\n",
    "\n",
    "# Train Random Forest Model\n",
    "rf = RandomForestClassifier(labelCol=target_column + \"_index\", featuresCol=\"features\", numTrees=100, maxDepth=5)\n",
    "rf_model = rf.fit(train_df)\n",
    "\n",
    "# Make Predictions\n",
    "predictions = rf_model.transform(test_df)\n",
    "\n",
    "# Step 3: Evaluate the Model\n",
    "\n",
    "# Initialize evaluator for different metrics\n",
    "evaluator_accuracy = MulticlassClassificationEvaluator(\n",
    "    labelCol=target_column + \"_index\", predictionCol=\"prediction\", metricName=\"accuracy\"\n",
    ")\n",
    "evaluator_precision = MulticlassClassificationEvaluator(\n",
    "    labelCol=target_column + \"_index\", predictionCol=\"prediction\", metricName=\"weightedPrecision\"\n",
    ")\n",
    "evaluator_recall = MulticlassClassificationEvaluator(\n",
    "    labelCol=target_column + \"_index\", predictionCol=\"prediction\", metricName=\"weightedRecall\"\n",
    ")\n",
    "\n",
    "# Calculate accuracy, precision, and recall\n",
    "accuracy = evaluator_accuracy.evaluate(predictions)\n",
    "precision = evaluator_precision.evaluate(predictions)\n",
    "recall = evaluator_recall.evaluate(predictions)\n",
    "\n",
    "print(f\"Accuracy: {accuracy:.2f}\")\n",
    "print(f\"Precision: {precision:.2f}\")\n",
    "print(f\"Recall: {recall:.2f}\")\n",
    "\n",
    "# Stop the Spark session\n",
    "spark.stop()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
